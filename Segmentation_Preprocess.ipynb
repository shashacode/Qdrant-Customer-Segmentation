{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2R94wf_gk6R",
        "outputId": "1eca6dd8-9709-4575-8652-c9508d36bbea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "SiFj2ZR9gnTi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "customers = pd.read_csv(\"/content/drive/My Drive/SuperTeam_Write/customer_details.csv\")\n",
        "purchases = pd.read_csv(\"/content/drive/My Drive/SuperTeam_Write/purchases.csv\")\n",
        "sessions = pd.read_csv(\"/content/drive/My Drive/SuperTeam_Write/sessions.csv\")\n",
        "reviews = pd.read_csv(\"/content/drive/My Drive/SuperTeam_Write/reviews.csv\")"
      ],
      "metadata": {
        "id": "fCNga4aZgt9q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_org = customers.copy()\n",
        "sessions_org = sessions.copy()\n",
        "purchases_org = purchases.copy()\n",
        "reviews_org = reviews.copy()"
      ],
      "metadata": {
        "id": "JqcznEL9g0WT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "# Initialize encoders and scalers\n",
        "label_encoder = LabelEncoder()\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Encode categorical features\n",
        "customers['gender'] = label_encoder.fit_transform(customers['gender'])\n",
        "sessions['page_viewed'] = label_encoder.fit_transform(sessions['page_viewed'])\n",
        "\n",
        "# Normalize numeric features\n",
        "customers[['age']] = scaler.fit_transform(customers[['age']])\n",
        "sessions[['view_duration']] = scaler.fit_transform(sessions[['view_duration']])\n",
        "purchases[['purchase_amount']] = scaler.fit_transform(purchases[['purchase_amount']])"
      ],
      "metadata": {
        "id": "j69QrY14g4jl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load the reviews dataset\n",
        "reviews_df = pd.read_csv('/content/drive/My Drive/SuperTeam_Write/reviews.csv')\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to convert text to BERT embeddings and return as a formatted string\n",
        "def get_bert_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', max_length=128, truncation=True, padding='max_length')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Get the embedding and convert it to a formatted string\n",
        "    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy().tolist()\n",
        "    return str(embedding)\n",
        "\n",
        "# Apply the embedding function to the review_text column\n",
        "reviews_df['review_embedding'] = reviews_df['review_text'].apply(get_bert_embedding)"
      ],
      "metadata": {
        "id": "u_vw8ZO2voaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23aa4a60-c7d7-4465-d3c6-26566acc3102"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Convert the review embeddings from string to numpy array\n",
        "reviews_df['review_embedding'] = reviews_df['review_embedding'].apply(\n",
        "    lambda x: np.fromstring(x.strip(\"[]\"), sep=',')\n",
        ")\n",
        "\n",
        "# Aggregate the review embeddings for each customer by taking the mean of their embeddings\n",
        "customer_embeddings = reviews_df.groupby('customer_id')['review_embedding'].apply(np.mean).reset_index()\n",
        "\n",
        "# Grouping by customer_id and aggregating numeric columns only\n",
        "# Sessions\n",
        "numeric_sessions_df = sessions.select_dtypes(include=[np.number])\n",
        "sessions_agg_df = sessions[['customer_id']].join(numeric_sessions_df).groupby('customer_id').mean().reset_index()\n",
        "\n",
        "# Purchases\n",
        "numeric_purchases_df = purchases.select_dtypes(include=[np.number])\n",
        "purchases_agg_df = purchases[['customer_id']].join(numeric_purchases_df).groupby('customer_id').mean().reset_index()\n",
        "\n",
        "# Merge customer embeddings with other customer features\n",
        "merged_df = customers.merge(customer_embeddings, on='customer_id', how='left')\n",
        "merged_df = merged_df.merge(sessions_agg_df, on='customer_id', how='left')\n",
        "merged_df = merged_df.merge(purchases_agg_df, on='customer_id', how='left')\n",
        "\n",
        "# Fill any missing values that resulted from the merge\n",
        "merged_df = merged_df.fillna(0)\n",
        "\n",
        "# Combine all features into a single vector for each customer\n",
        "feature_columns = ['age', 'gender', 'view_duration', 'purchase_amount', 'review_embedding']\n",
        "merged_df['combined_vector'] = merged_df.apply(\n",
        "    lambda row: np.concatenate([row['review_embedding'], [row['age'], row['gender'], row['view_duration'], row['purchase_amount']]]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Convert the combined_vector and review_embedding back to string format\n",
        "merged_df['combined_vector'] = merged_df['combined_vector'].apply(lambda x: str(list(x)))\n",
        "merged_df['review_embedding'] = merged_df['review_embedding'].apply(lambda x: str(list(x)))\n",
        "\n",
        "# Display the first few rows after combining vectors\n",
        "print(merged_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOvRri7YcXw2",
        "outputId": "7965058e-53a8-48f5-f254-4ae8a57d0930"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  customer_id       age  gender            location signup_date  \\\n",
            "0     CUST001  0.723404       0         Robertsbury  2023-11-21   \n",
            "1     CUST002  0.468085       1     Lake Heidimouth  2023-08-10   \n",
            "2     CUST003  0.212766       1  South Stephenville  2024-05-25   \n",
            "3     CUST004  0.957447       1         Brandyshire  2023-11-05   \n",
            "4     CUST005  0.106383       0          Wrightberg  2023-03-10   \n",
            "\n",
            "                                    review_embedding  page_viewed  \\\n",
            "0  [0.3026423727472623, -0.20583192942043146, 0.3...     5.800000   \n",
            "1  [-0.027535068492094677, -0.2214363068342209, 0...     7.454545   \n",
            "2  [0.09626923501491547, -0.21890243291854858, 0....    17.500000   \n",
            "3  [0.011832010932266712, -0.29787568747997284, 0...     2.125000   \n",
            "4  [0.4246213883161545, -0.2865287885069847, 0.23...    14.428571   \n",
            "\n",
            "   view_duration  purchase_amount  \\\n",
            "0       0.443051         0.356144   \n",
            "1       0.536518         0.438716   \n",
            "2       0.611582         0.663063   \n",
            "3       0.520551         0.425212   \n",
            "4       0.514044         0.367323   \n",
            "\n",
            "                                     combined_vector  \n",
            "0  [0.3026423727472623, -0.20583192942043146, 0.3...  \n",
            "1  [-0.027535068492094677, -0.2214363068342209, 0...  \n",
            "2  [0.09626923501491547, -0.21890243291854858, 0....  \n",
            "3  [0.011832010932266712, -0.29787568747997284, 0...  \n",
            "4  [0.4246213883161545, -0.2865287885069847, 0.23...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply PCA to reduce the dimensionality of the combined vectors\n",
        "pca = PCA(n_components=5)  # Set n_components to a feasible value, in this case 5\n",
        "reduced_vectors = pca.fit_transform(np.stack(merged_df['combined_vector'].apply(eval)))\n",
        "\n",
        "# Assign the reduced vectors to a new column and convert to JSON-like string format\n",
        "merged_df['reduced_vector'] = reduced_vectors.tolist()\n",
        "merged_df['reduced_vector'] = merged_df['reduced_vector'].apply(lambda x: str(x))\n",
        "\n",
        "# Display the first few rows after adding the reduced vectors\n",
        "print(merged_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZDtzac0eFof",
        "outputId": "8249706c-06bf-4621-b1ec-ca91ebf6e959"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  customer_id       age  gender            location signup_date  \\\n",
            "0     CUST001  0.723404       0         Robertsbury  2023-11-21   \n",
            "1     CUST002  0.468085       1     Lake Heidimouth  2023-08-10   \n",
            "2     CUST003  0.212766       1  South Stephenville  2024-05-25   \n",
            "3     CUST004  0.957447       1         Brandyshire  2023-11-05   \n",
            "4     CUST005  0.106383       0          Wrightberg  2023-03-10   \n",
            "\n",
            "                                    review_embedding  page_viewed  \\\n",
            "0  [0.3026423727472623, -0.20583192942043146, 0.3...     5.800000   \n",
            "1  [-0.027535068492094677, -0.2214363068342209, 0...     7.454545   \n",
            "2  [0.09626923501491547, -0.21890243291854858, 0....    17.500000   \n",
            "3  [0.011832010932266712, -0.29787568747997284, 0...     2.125000   \n",
            "4  [0.4246213883161545, -0.2865287885069847, 0.23...    14.428571   \n",
            "\n",
            "   view_duration  purchase_amount  \\\n",
            "0       0.443051         0.356144   \n",
            "1       0.536518         0.438716   \n",
            "2       0.611582         0.663063   \n",
            "3       0.520551         0.425212   \n",
            "4       0.514044         0.367323   \n",
            "\n",
            "                                     combined_vector  \\\n",
            "0  [0.3026423727472623, -0.20583192942043146, 0.3...   \n",
            "1  [-0.027535068492094677, -0.2214363068342209, 0...   \n",
            "2  [0.09626923501491547, -0.21890243291854858, 0....   \n",
            "3  [0.011832010932266712, -0.29787568747997284, 0...   \n",
            "4  [0.4246213883161545, -0.2865287885069847, 0.23...   \n",
            "\n",
            "                                      reduced_vector  \n",
            "0  [-0.7232226342878398, 0.6884320538860841, 0.39...  \n",
            "1  [-0.13744552695395318, -0.23925832254305657, 1...  \n",
            "2  [-0.29472834360582284, -0.0718849311135858, 0....  \n",
            "3  [-1.0207225295509013, -0.966953093291974, -0.3...  \n",
            "4  [-0.23915018077988823, 1.1210631777895994, -0....  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the dataframe to a csv file as the normalized data for clustering\n",
        "merged_df.to_csv(\"/content/drive/My Drive/SuperTeam_Write/customers_vect.csv\", index=False)"
      ],
      "metadata": {
        "id": "FI8Mqlxgu2bm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merging the original customer information with the new features for the customer similarity search\n",
        "df = customer_org.merge(merged_df[['customer_id', 'page_viewed', 'view_duration', 'purchase_amount', 'review_embedding', 'combined_vector', 'reduced_vector', 'review_embedding']], on='customer_id', how='left')"
      ],
      "metadata": {
        "id": "gVqNg39FsNFb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save to csv file\n",
        "df.to_csv(\"/content/drive/My Drive/SuperTeam_Write/customers.csv\", index=False)"
      ],
      "metadata": {
        "id": "WrRJIhjauSGK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L-lPN_5gunGg"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}